





# ğŸš• NYC Taxi Real-Time Data Pipeline (Kafka â†’ Snowflake)

This project demonstrates a complete **real-time Data Engineering pipeline** using **Kafka, Python, Docker, and Snowflake**.  
NYC Yellow Taxi trip data is streamed, cleaned in real time, and loaded into Snowflake for analytics and reporting.

---

## ğŸ¯ Objective

To build an end-to-end streaming pipeline that:
- Extracts data from a **MySQL source database**
- Streams records to **Apache Kafka**
- Cleans data **message by message**
- Loads clean records into **Snowflake**
- Handles large-scale data efficiently using batching

---

## ğŸ§° Tools & Skills Used

- **Python** (kafka-python, mysql-connector, Snowflake Connector)
- **Apache Kafka & Zookeeper**
- **MySQL** (Source system)
- **Snowflake** (Cloud Data Warehouse)
- **Docker & Docker Compose**
- **SQL**
- **Git & GitHub**

---

## âœ… Key Features

- âš™ï¸ Real-time streaming pipeline (MySQL â†’ Kafka â†’ Snowflake)  
- ğŸ§¹ Record-level data cleaning before ingestion  
- ğŸ“¦ Batch inserts for high-performance Snowflake loading  
- ğŸ§  Fault-tolerant consumer with bad-record handling  
- ğŸ³ Fully Dockerized Kafka & Zookeeper setup  
- ğŸ“Š Production-ready project structure  

---

## ğŸ§¹ Data Cleaning Logic

Implemented in `utils/data_cleaner.py`

- Drop records with missing pickup/dropoff timestamps  
- Remove trips with invalid or zero distance  
- Filter out negative or invalid fare values  
- Convert timestamps to Snowflake-compatible format  
- Safely cast numeric columns  
- Skip invalid records without stopping the pipeline  

---

## ğŸ“Š Source & Target Tables

### Source: MySQL
- **Table:** `yellow_taxi_trips`

### Kafka
- **Topic:** `nyc_taxi_raw`

### Target: Snowflake
- **Database:** `NYC_TAXI_PROJECT`
- **Schema:** `CLEAN`
- **Table:** `YELLOW_TAXI_CLEAN`

**Target Columns:**
- `VENDOR_ID`
- `PICKUP_TIME`
- `DROPOFF_TIME`
- `PASSENGER_COUNT`
- `TRIP_DISTANCE`
- `PICKUP_LOCATION`
- `DROPOFF_LOCATION`
- `PAYMENT_TYPE`
- `FARE_AMOUNT`
- `TOTAL_AMOUNT`

---

## ğŸ“‚ Project Folder Structure

nyc_taxi_pipeline/
â”‚
â”œâ”€â”€ producer/
â”‚ â””â”€â”€ mysql_kafka_producer.py # MySQL â†’ Kafka producer
â”‚
â”œâ”€â”€ consumer/
â”‚ â””â”€â”€ kafka_snowflake_consumer.py # Kafka â†’ Snowflake consumer
â”‚
â”œâ”€â”€ utils/
â”‚ â””â”€â”€ data_cleaner.py # Data validation & cleaning logic
â”‚
â”œâ”€â”€ config/
â”‚ â””â”€â”€ settings.py # Kafka, MySQL, Snowflake configs
â”‚
â”œâ”€â”€ docker-compose.yml # Kafka & Zookeeper setup
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore


---

## ğŸ–¥ï¸ How to Run


Step 1 â€” Start Kafka & Zookeeper
docker compose up -d

Step 2 â€” Run Kafka Producer
python -m nyc_taxi_pipeline.producer.mysql_kafka_producer

Step 3 â€” Run Kafka Consumer
python -m nyc_taxi_pipeline.consumer.kafka_snowflake_consumer

---

## âš™ï¸ Performance & Tuning

- Batch size of **500 records** for efficient Snowflake inserts  
- Kafka **consumer group support** for scalability  
- **JSON serialization** for lightweight message transfer  
- **Auto offset management** handled by Kafka  
- Optimized Snowflake inserts using `executemany()`  

---

## ğŸ¯ Project Use Cases

- Real-time data ingestion pipelines  
- Kafka-based **ETL / ELT** systems  
- Snowflake streaming ingestion workflows  
- Data Engineering interview showcase project  
- Handling **millions of records** efficiently  

---

## ğŸš€ Future Enhancements

- Dead Letter Queue (DLQ) for invalid records  
- Kafka topic partitioning for higher scalability  
- Airflow orchestration for pipeline scheduling  
- Monitoring & alerting (consumer lag, throughput)  
- Snowflake `COPY INTO` optimization using staged files  

---



























































































































































































































































































































































































































































































































































































































































































